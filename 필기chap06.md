# Chapter 6. 추정(Estimation)의 이해

통계학의 핵심 목표 중 하나는  
**표본을 이용해 모집단의 특성을 추정하는 것**이다.

이 장에서는 ‘추정량이란 무엇인가’, ‘좋은 추정량은 어떤 기준을 만족해야 하는가’를  
ADP 시험 및 실무 분석에서 바로 사용할 수 있도록 설명한다.

---

# 🟥 6.1 추정이란 무엇인가?

현실에서는 전체 모집단의 데이터를 얻는 것이 거의 불가능하다.  
따라서 우리가 할 수 있는 일은 **표본을 통해 모집단의 특성을 추정하는 것**이다.

- 모집단 평균 μ → 표본평균 x̄  
- 모집단 분산 σ² → 표본분산 s²  
- 모집단 비율 p → 표본비율 p̂  

즉, 우리가 계산하는 대부분의 통계량은  
모집단의 ‘근사치(estimate)’로 사용된다.

---

# 🟥 6.2 점추정(Point Estimation)과 구간추정(Interval Estimation)

추정은 크게 두 가지 유형으로 나뉜다.

---

## 🔷 6.2.1 점추정(Point Estimate)

모집단 모수(parameter)의 값을 **하나의 숫자로** 추정하는 방식.

예:
- 평균 키가 171.2cm다.  
- 구매 전환율이 7.5%다.  
- 평균 대기시간이 12.4분이다.

점추정은 간단하지만 **불확실성**이 존재한다.

---

## 🔷 6.2.2 구간추정(Interval Estimate)

점추정 대신  
**모수의 범위를 제시하는 방식.**

예:  
“평균 키는 170.3 ~ 172.5cm 범위에 있을 가능성이 95%입니다.”

신뢰구간(confidence interval)은  
점추정의 불확실성을 수량화한 결과물이다.

---

# 🟥 6.3 좋은 추정량의 조건 (ADP 필수 암기)

추정량의 성능은 다음 기준으로 평가한다.

---

## 🔷 1) 불편성(Unbiasedness)

추정량의 기대값이 모수와 같아야 한다.

\[
E(\hat{\theta}) = \theta
\]

예:
- 표본 평균 x̄는 모집단 평균 μ의 불편추정량
- 표본 비율 p̂는 모집단 비율 p의 불편추정량

불편하지 않은 추정량은  
반복 추출했을 때 **평균적으로 정확**하다는 의미이다.

---

## 🔷 2) 일치성(Consistency)

표본 크기가 커질수록 추정량이 모수에 가까워져야 한다.

\[
\hat{\theta} \to \theta \;\; (n \to \infty)
\]

즉, 데이터를 많이 모으면 추정이 점점 정확해진다.

---

## 🔷 3) 효율성(Efficiency)

추정량이 여러 개 있을 때,
가장 **분산이 작은 추정량**이 더 효율적이다.

예:
- 두 추정량 A, B의 기대값이 같다면  
  → **분산이 더 작은 추정량**이 더 좋은 추정량

---

## 🔷 4) 충분성(Sufficiency)

추정량이 데이터의 모든 정보를 활용하고 있는가?

예:
- 정규분포에서 μ의 충분통계량은 표본평균 x̄  
- 불필요한 정보 손실이 없음

---

# 🟥 6.4 신뢰구간(Confidence Interval, CI)

신뢰구간은 추정의 불확실성을 제시하는 가장 중요한 개념이다.

예:  
“μ는 95% 확률로 50.2 ~ 52.8 사이에 있다”는 뜻이 아니다.

올바른 해석:
> 동일한 방법으로 여러 표본을 뽑아 신뢰구간을 만들면,  
> 그 중 95%가 모집단 평균 μ를 포함한다.

---

## 🔷 6.4.1 평균의 신뢰구간 (모분산을 알고 있는 경우)

\[
\bar{x} \pm Z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
\]

예:  
Z₀․₂₅ = 1.96 (95% 신뢰수준)

---

## 🔷 6.4.2 평균의 신뢰구간 (모분산을 모르는 경우: t 사용)

\[
\bar{x} \pm t_{\alpha/2, (n-1)} \frac{s}{\sqrt{n}}
\]

t-분포를 사용해야 하는 이유:  
단일 표본의 분산 추정은 불확실성이 크기 때문.

---

## 🔷 6.4.3 비율의 신뢰구간

\[
\hat{p} \pm Z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\]

선거 지지율 발표에 항상 “표본오차 ±2.5%”가 따라붙는 이유가 이 공식이다.

---

# 🟥 6.5 표본크기 결정 (Sample Size Determination)

ADP 필기에서 매우 자주 나오는 개념.

원하는 오차범위(E)와 신뢰수준을 만족하려면  
필요한 표본 크기는 다음으로 계산된다.

---

## 🔷 6.5.1 평균에 대한 표본 크기

\[
n = \left( \frac{Z_{\alpha/2} \sigma}{E} \right)^2
\]

---

## 🔷 6.5.2 비율에 대한 표본 크기

\[
n = \frac{Z_{\alpha/2}^2 \hat{p}(1-\hat{p})}{E^2}
\]

예:
- 표본오차 ±3% (E=0.03)
- 95% 신뢰수준 (Z=1.96)
- p̂ ≈ 0.5 (최대 분산 가정)

\[
n = \frac{1.96^2 \cdot 0.5(1-0.5)}{0.03^2}
\approx 1067
\]

즉, 대부분의 여론조사 표본 수가 1,000명인 이유다.

---

# 🟥 6.6 추정량 비교에서 자주 하는 실수

1) “불편추정량”이 항상 좋은 것은 아니다.  
2) 효율성이 높은데 더 복잡한 경우도 있다.  
3) 표본 크기가 매우 작을 때는 t-분포를 반드시 사용해야 한다.  
4) 구간추정의 해석을 ‘확률 95%로 μ가 이 안에 있다’라고 오해함.  

---

# ✔ Chapter 6 요약

- 추정은 표본을 이용해 모집단을 예측하는 과정  
- 점추정은 단일 값, 구간추정은 범위를 제시  
- 좋은 추정량은 불편성·일치성·효율성·충분성을 만족  
- 신뢰구간은 불확실성을 수량화하는 핵심 도구  
- 표본 크기 결정은 ADP 필기에서 매우 자주 출제됨  
- 통계적 추정의 이해 없이는 가설검정·회귀분석 수행이 불가능



# Chapter 7. 가설검정(Hypothesis Testing)의 이해

가설검정은 통계학에서 가장 중요한 개념 중 하나이며  
ADP 필기·실기, 그리고 실무 분석 모두에서 핵심적으로 사용된다.

가설검정은 “우리가 관측한 결과가 우연인가, 아니면 의미 있는 차이인가?”를  
통계적으로 판단하는 절차이다.

---

# 🟥 7.1 가설검정이란 무엇인가?

가설검정은 모집단에 대한 주장을 검증하는 절차다.

예:
- 신약은 기존 약보다 효과가 좋은가?
- 광고 A와 광고 B 중 어떤 것이 더 효과적인가?
- 고객 이탈률이 5%에서 변했는가?
- 제품의 평균 무게가 500g인가?

가설검정은 다음 질문에 답한다:

> “데이터가 말해주는 차이가 통계적으로 유의미한가?”

---

# 🟥 7.2 귀무가설 H₀와 대립가설 H₁

가설검정은 두 가지 가설을 세우는 것에서 시작한다.

---

## 🔷 7.2.1 귀무가설 (Null Hypothesis, H₀)
“차이가 없다”, “효과가 없다”라는 기본 가정.

예:
- 평균 μ = 50  
- 신약과 기존 약은 효과 차이가 없다  
- 광고 전후 구매 변화가 없다  

이는 일종의 **기본 믿음(default)** 이다.

---

## 🔷 7.2.2 대립가설 (Alternative Hypothesis, H₁)
검정하고 싶은 주장.

예:
- 평균 μ ≠ 50 (양측)
- 평균 μ > 50 (우측)
- 평균 μ < 50 (좌측)

대립가설은 “우리가 증명하고 싶은 것”이다.

---

# 🟥 7.3 검정통계량(Test Statistic)

표본 데이터를 기반으로  
“귀무가설이 맞다면 이 정도의 결과가 얼마나 희귀한가?”를 계산하는 값.

검정통계량의 종류:

| 목적 | 사용 통계량 |
|------|-------------|
| 평균 검정 | z, t |
| 비율 검정 | z |
| 분산 검정 | χ² |
| 두 분산 비교 | F |
| 회귀계수 검정 | t |
| 모형 전체 유의성 | F |

---

# 🟥 7.4 유의수준(α)과 p-value

가설검정의 핵심은 두 가지 값이다.

---

## 🔷 7.4.1 유의수준 α (Significance Level)

귀무가설이 참인데도 기각할 **오류를 허용하는 최대 확률**.

일반적으로:
- α = 0.05  
- α = 0.01  
- α = 0.10  

예:  
“우리는 5%까지 오류를 허용하겠다.”

---

## 🔷 7.4.2 p-value

p-value는 다음을 의미한다:

> “귀무가설이 맞다고 가정할 때,  
> 우리가 얻은 표본 이상의 극단적인 결과가 나올 확률.”

- **p < α → 귀무가설 기각 (유의)**  
- **p ≥ α → 귀무가설 채택(기각 실패)**  

### p-value에 대한 흔한 오해
- p-value = H₀가 참일 확률 → ❌  
- p-value가 작으면 차이가 “크다” → ❌  
- 표본 크기가 크면 작은 차이도 유의해짐 → ⭕ (중요)

---

# 🟥 7.5 1종 오류와 2종 오류

가설검정에는 두 가지 종류의 오류가 존재한다.

---

## 🔷 1종 오류 (Type I Error)

H₀가 참인데 기각하는 오류.  
(= false positive)

확률: α

예:
- 실제로는 무죄인 사람을 유죄 판결  
- 실제로 효과 없는 신약을 효과가 있다고 판단

---

## 🔷 2종 오류 (Type II Error)

H₀가 거짓인데 기각하지 못하는 오류.  
(= false negative)

확률: β

예:
- 실제로 효과가 있는 신약을 효과 없다고 판단  
- 고객 이탈이 증가했는데 변화 없다고 판단  

### 검정력(Power)
\[
Power = 1 - \beta
\]

검정력은 “진짜 차이를 발견할 능력”을 의미한다.

---

# 🟥 7.6 양측검정 vs 단측검정

### ● 양측검정  
“차이가 있는가?”  
\[
H_1 : \mu \neq \mu_0
\]

### ● 우측검정  
“증가했는가?”  
\[
H_1 : \mu > \mu_0
\]

### ● 좌측검정  
“감소했는가?”  
\[
H_1 : \mu < \mu_0
\]

다음 상황일 때 단측을 사용한다:

- 신약이 기존 약보다 “더 좋아야 함”
- 신규 시스템이 기존보다 “더 빠르길 기대”
- 지지율이 “기존보다 상승했는가”

중요: 단측검정은 근거 없이 사용하면 안 된다.

---

# 🟥 7.7 실제 검정 절차 (ADP 실무형 정리)

1) 가설 설정  
2) 유의수준 α 선택  
3) 검정통계량 계산  
4) p-value 계산  
5) 귀무가설 기각 여부 판단  
6) 실질적 의미 해석까지 포함해야 함

---

# 🟥 7.8 평균의 검정 (z-test, t-test)

---

## 🔷 7.8.1 모분산을 알고 있는 경우 → z-test

\[
Z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}
\]

예:
- 표본평균이 52.1일 때  
- μ₀ = 50  
- σ = 5  
- n = 40

---

## 🔷 7.8.2 모분산을 모르는 경우 → t-test

\[
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
\]

t는 표본이 작을 때 반드시 사용해야 한다.

---

# 🟥 7.9 두 집단 평균 비교

---

## 🔷 독립표본 t-test  
두 집단이 서로 다른 사람일 때.

예:
- 남학생 vs 여학생 평균 점수  
- 광고 A vs 광고 B CTR

## 🔷 대응표본 t-test  
같은 사람을 두 번 측정.

예:
- 다이어트 전후 체중  
- 교육 프로그램 전후 평가  

대응표본은 **변동성이 감소하여 검정력이 증가**한다.

---

# 🟥 7.10 비율 검정 (Proportion Test)

\[
Z = \frac{\hat{p} - p_0}{\sqrt{p_0(1-p_0)/n}}
\]

예:
- 구매 비율이 7%에서 변화했는가?  
- 웹사이트 전환율이 3%를 넘는가?

---

# 🟥 7.11 분산 검정 (Variance Test)

### ● 카이제곱 검정
모집단 분산이 σ₀²인지 검정.

\[
\chi^2 = \frac{(n-1)s^2}{\sigma_0^2}
\]

### ● 두 집단 분산 비교 → F-test
\[
F = \frac{s_1^2}{s_2^2}
\]

---

# 🟥 7.12 가설검정에서 흔히 하는 실수

1) p-value < 0.05이면 무조건 의미 있다고 단정  
2) 효과 크기를 확인하지 않음  
3) 표본 크기가 클수록 “사소한 차이도 유의해질 수 있음”을 무시  
4) α와 β의 균형을 고려하지 않음  
5) 단측검정을 남용  

---

# ✔ Chapter 7 요약

- 가설검정은 “데이터의 차이가 우연인가 의미 있는가?”를 판단하는 절차  
- 귀무가설(H₀)은 “변화 없음”, 대립가설(H₁)은 “변화 있음”  
- 유의수준 α, p-value는 검정의 핵심  
- 1종 오류(α), 2종 오류(β), 검정력(power)를 이해해야 함  
- 평균·비율·분산 검정 모두 표본분포 개념 위에서 작동  
- 실제로는 p-value뿐 아니라 효과 크기(ES)를 해석해야 함  




# Chapter 8. 분산분석(ANOVA)의 이해

분산분석(ANOVA: Analysis of Variance)은  
**세 개 이상 집단의 평균을 비교해야 할 때** 사용하는 통계 기법이다.

t-검정이 두 집단의 비교라면,  
ANOVA는 그 확장으로 볼 수 있다.

예:
- 3개 도시의 평균 매출 비교  
- 4가지 약물의 효과 비교  
- 5개 광고 캠페인의 성과 비교  
- 품질 검사에서 3개 공장의 평균 불량률 비교

---

# 🟥 8.1 왜 ANOVA가 필요한가?

두 집단 비교 → t-test로 충분  
세 집단 이상 → t-test를 반복하면?

문제 발생:
1) 검정을 여러 번 하면 → **1종 오류 증가**  
2) 전체적인 차이를 먼저 판단할 필요가 있음  
3) t-test는 pairwise 비교만 가능

ANOVA는 “집단 간 차이가 있는지를 전체적으로 먼저 판단”하는 절차를 제공한다.

---

# 🟥 8.2 ANOVA의 기본 아이디어

ANOVA는 다음 두 가지 변동을 비교한다:

1) **집단 간 변동(Between-group variance)**  
2) **집단 내 변동(Within-group variance)**

핵심 질문:

> 집단 간 평균의 차이가  
> 집단 내의 자연스러운 변동보다 충분히 큰가?

이 비율로 계산된 것이 **F-통계량**이다.

---

# 🟥 8.3 F-통계량

ANOVA의 검정통계량은 F-분포를 따른다.

\[
F = \frac{MS_{\text{between}}}{MS_{\text{within}}}
\]

### ● MSbetween = 집단 간 평균제곱  
→ 집단 간 평균 차이를 반영  
### ● MSwithin = 집단 내 평균제곱  
→ 개인별 자연스러운 변동 반영

F가 커질수록:
- 집단 간 차이가 크다  
- 유의할 가능성이 높다  

---

# 🟥 8.4 일원분산분석 (One-way ANOVA)

요인이 1개일 때 사용.

예:
- 도시별 평균 매출 비교  
- 약물 종류별 평균 반응 비교  
- 교육 프로그램 A/B/C의 평균 점수 비교  

---

## 🔷 8.4.1 가설 설정

\[
H_0: \mu_1 = \mu_2 = ... = \mu_k
\]

\[
H_1: \text{적어도 하나의 평균은 다르다}
\]

즉, 어느 집단이 다른지 명시하지 않는다.

---

## 🔷 8.4.2 검정 절차

1) F-통계량 계산  
2) 자유도 (df₁ = k-1, df₂ = N-k)  
3) F-분포 기준으로 p-value 계산  
4) p < α이면 H₀ 기각  

---

## 🔷 8.4.3 ANOVA는 “어디가 다른지” 알려주지 않는다

ANOVA는 말해주는 것:

- “집단 간 평균의 차이가 존재함”

하지만 말해주지 않는 것:

- “어떤 집단과 어떤 집단이 다른가?”

이를 확인하려면 **사후검정(Post-hoc Test)**이 필요하다.

---

# 🟥 8.5 사후검정(Post-hoc Tests)

ANOVA에서 H₀를 기각했다면 다음 단계는 “구체적으로 어떤 집단이 다른가?”를 찾는 것이다.

---

## 🔷 8.5.1 Tukey HSD (가장 대표적)

모든 쌍(pair)을 비교한다.

장점:
- 보수적이며 안정적  
- 대부분의 실무에서 사용  

---

## 🔷 8.5.2 Bonferroni

다중비교의 p-value를 조정.

장점: 오류 통제 강함  
단점: 너무 보수적이어서 검정력이 떨어질 수 있음

---

## 🔷 8.5.3 Scheffé

가장 보수적이지만 매우 안전한 방법.

---

# 🟥 8.6 이원분산분석 (Two-way ANOVA)

요인이 2개일 때 사용한다.

예:
- 약물 종류(요인 A) + 투여량(요인 B)  
- 교육 방법(A) + 시험 난이도(B)  
- 매장 위치(A) + 광고 유형(B)

이원분산분석은 다음 효과를 동시에 검정한다:

1) A 주효과  
2) B 주효과  
3) A×B 교호작용 (interaction)

---

### 🔷 교호작용이란?

두 요인이 **서로 영향을 주며 결합된 효과**를 가지는 경우.

예:  
약 A는 10mg에서 효과가 좋지만,  
50mg에서는 오히려 효과가 떨어질 수 있음.

이런 관계는 단순 평균 비교로는 발견할 수 없다.

---

# 🟥 8.7 ANOVA의 가정(Assumptions)

ANOVA가 정확하게 작동하려면 다음 조건들이 필요하다.

---

## 🔷 1) 독립성(Independence)

각 데이터가 서로 독립이어야 한다.  
예: 반복 측정 데이터를 독립표본으로 분석하는 오류 → 매우 흔한 실수

---

## 🔷 2) 정규성(Normality)

각 집단의 데이터가 정규분포를 따라야 한다.  
하지만 표본 크기 n ≥ 30이면 중심극한정리에 의해 크게 문제 되지 않음.

Shapiro-Wilk, Q-Q plot로 확인 가능.

---

## 🔷 3) 등분산성(Homogeneity of variance)

집단 간 분산이 비슷해야 한다.

Levene test 또는 Bartlett test로 검정.

### 등분산이 깨지면?
- Welch ANOVA 사용 (등분산 가정 불필요)  
- 또는 비모수 검정(Kruskal-Wallis) 사용  

---

# 🟥 8.8 ANOVA와 비모수 검정 비교

정규성 또는 등분산이 크게 위배될 경우  
비모수 검정을 사용한다.

| 목적 | 등분산·정규성 만족 | 불만족 |
|------|------------------|--------|
| 3개 이상 집단 비교 | ANOVA | Kruskal-Wallis |

Kruskal-Wallis는 평균 대신 **순위(rank)** 기반으로 비교한다.

---

# 🟥 8.9 ANOVA 실무 해석 포인트

1) F-통계량이 크고 p<0.05 → 집단 간 차이 있음  
2) 하지만 어디가 다른지는 반드시 사후검정 필요  
3) 효과크기(effect size)를 함께 보는 것이 바람직  
   - 예: η², partial η²  
4) 단위가 다른 경우 데이터 표준화 필요  
5) 이상치(outlier)에 매우 민감  
6) 불균형 데이터(unbalanced design)에서는 조심 필요  

---

# ✔ Chapter 8 요약

- ANOVA는 3개 이상 집단의 평균을 비교하는 통계 기법  
- F-통계량은 집단 간 변동 / 집단 내 변동의 비율  
- 일원·이원 ANOVA로 구분  
- ANOVA는 “차이가 있다/없다”만 말함 → 사후검정 필요  
- 가정: 독립성, 정규성, 등분산성  
- 가정 위배 시 Welch ANOVA 또는 Kruskal-Wallis 사용  



# Chapter 9. 시계열 분석(Time Series Analysis)의 기초

시계열 분석은 시간에 따라 변화하는 데이터를 분석하는 통계·예측 기법이다.  
분류·회귀와 달리 **데이터가 시간 의존성을 갖고 있다는 점**이 특징이다.

대표적 활용 사례:
- 주가 예측  
- 매출 예측  
- 수요 예측  
- 기상 변화 분석  
- 서버 트래픽 예측  
- 환율/금리 분석  

ADP 실기에서도 자주 등장하는 중요한 파트다.

---

# 🟥 9.1 시계열 데이터의 특징

일반 데이터와 다른 점:

## 🔷 1) 시간 순서가 매우 중요
데이터를 섞으면 구조가 완전히 무너진다.

## 🔷 2) 자기상관(Autocorrelation) 존재
이전 값이 이후 값에 영향을 준다.

## 🔷 3) 추세(Trend)
시간이 지날수록 증가하거나 감소하는 패턴.

## 🔷 4) 계절성(Seasonality)
특정 주기마다 반복되는 규칙적 패턴.

예:
- 월요일 매출은 낮고 주말 매출은 높음  
- 여름 전기 사용량 증가  
- 매년 설 연휴 전에 택배 물량 증가  

시계열 모델은 이러한 구조를 이해하고 예측하는 도구이다.

---

# 🟥 9.2 시계열 구성 요소(Time Series Components)

시계열은 크게 네 가지 요소로 나뉜다.

1) **추세(Trend)** – 장기적인 상승/하락 패턴  
2) **계절성(Seasonality)** – 일정 주기의 반복  
3) **순환(Cycle)** – 경기처럼 불규칙한 장·단기 파동  
4) **불규칙(Noise)** – 설명할 수 없는 랜덤 변화  

시계열의 핵심 작업은  
“데이터에서 추세와 계절성을 제거(분해)하여 구조를 이해하는 것”이다.

---

# 🟥 9.3 정성적 분석과 정량적 분석

## 🔷 정성적 접근(Qualitative)
- 트렌드가 있는지 그래프로 확인  
- 계절성을 시각적으로 판단  
- 이상값(outlier) 탐지  

## 🔷 정량적 접근(Quantitative)
- ACF/PACF 분석  
- ARIMA 모델  
- 분해(Decomposition)  
- Exponential smoothing  
- 예측 오차(MAPE, RMSE) 계산  

---

# 🟥 9.4 ACF와 PACF

시계열 분석에서 가장 중요한 도구는  
**ACF(AutoCorrelation Function)** 와  
**PACF(Partial AutoCorrelation Function)** 이다.

---

## 🔷 9.4.1 ACF (자기상관함수)

\[
\rho_k = Corr(Y_t , Y_{t-k})
\]

즉, **k시차 전 값과 현재 값의 상관관계**를 의미한다.

ACF는 전체적인 상관 구조를 보여준다.

해석 예:
- ACF가 천천히 감소 → AR 모델 가능  
- ACF가 특정 시점에서 갑자기 끊김 → MA 모델 후보  

---

## 🔷 9.4.2 PACF (부분자기상관함수)

PACF는 “중간 시차의 영향을 제거한 뒤 남는 순수 상관관계”를 의미한다.

해석 예:
- PACF가 p 시점에서 끊김 → AR(p) 모델  
- ACF가 q 시점에서 끊김 → MA(q) 모델  

ACF·PACF는 ARIMA 모델 선택의 기초이다.

---

# 🟥 9.5 시계열 변환: 정상성(Stationarity)

ARIMA 모델을 적용하기 위해서는  
**시계열이 정상성(stationarity)을 가져야 한다.**

정상성 조건:
- 평균이 일정  
- 분산이 일정  
- 공분산이 시간에 의존하지 않음  

만약 추세·계절성이 존재하면?

→ 정상성을 위배  
→ **차분(differencing)** 을 통해 제거해야 한다.

---

## 🔷 9.5.1 차분(Differencing)

\[
Y_t' = Y_t - Y_{t-1}
\]

- 1차 차분 → 추세 제거  
- 계절 차분 → 주기적 패턴 제거  

ARIMA에서 I(d)는 차분 횟수를 뜻한다.

---

# 🟥 9.6 ARIMA 모델의 이해

ARIMA는 시계열 분석의 가장 기본이 되는 모델이다.

\[
ARIMA(p, d, q)
\]

- **p** : AR(자기회귀) 차수  
- **d** : 차분 횟수  
- **q** : MA(이동평균) 차수  

---

## 🔷 9.6.1 AR(p)

현재 값이 과거 p개 값과 선형 관계.

\[
Y_t = c + \sum_{i=1}^p \phi_i Y_{t-i} + \epsilon_t
\]

---

## 🔷 9.6.2 MA(q)

현재 값이 과거 q개 오차항과 관계.

\[
Y_t = c + \sum_{i=1}^q \theta_i \epsilon_{t-i} + \epsilon_t
\]

---

## 🔷 9.6.3 ARIMA(p,d,q) 모델링 절차

1) 시계열 그래프에서 추세·계절성 확인  
2) 차분(differencing)으로 정상성 확보  
3) ACF/PACF로 p, q 추정  
4) 모형 적합 후 잔차 분석  
5) 예측 수행  

---

# 🟥 9.7 계절형 ARIMA (SARIMA)

계절성을 포함한 ARIMA:

\[
SARIMA(p,d,q)(P,D,Q)_s
\]

s는 계절 주기(예: 12개월, 7일 등)

예:
- 월별 매출 → s=12  
- 일별 트래픽 → s=7  

---

# 🟥 9.8 지수평활법(Exponential Smoothing)

ARIMA와 더불어 중요한 시계열 예측 기법.

### ● 단순 지수평활 (SES)  
→ 추세 없고 계절성도 없는 데이터

### ● Holt (이중 지수평활)  
→ 추세 존재

### ● Holt-Winters (삼중 지수평활)  
→ 추세 + 계절성 존재

기업 실무에서 가장 자주 쓰이는 방식 중 하나.

---

# 🟥 9.9 예측 정확도 측정

시계열 예측 모델 평가 지표:

\[
RMSE = \sqrt{\frac{1}{n}\sum (y_t - \hat{y_t})^2}
\]

\[
MAPE = \frac{1}{n}\sum \left| \frac{y_t - \hat{y_t}}{y_t} \right| \times 100
\]

\[
MAE = \frac{1}{n}\sum |y_t - \hat{y_t}|
\]

---

# 🟥 9.10 시계열 분석 실무 포인트

1) 정상성 여부 확인이 가장 중요  
2) ACF/PACF 해석은 ARIMA 선택의 필수  
3) 차분 후 잔차가 백색잡음이면 모형 적합 성공  
4) 계절성이 강하면 SARIMA 또는 Holt-Winters  
5) 이상치(outlier)가 예측 정확도를 크게 떨어뜨림  
6) 시계열은 일반 회귀분석처럼 교란변수 통제가 어려움  
7) 데이터 길이가 부족하면 모델 안정성이 떨어짐  

---

# ✔ Chapter 9 요약

- 시계열은 시간 의존성을 가진 데이터  
- 추세, 계절성, 순환, 불규칙 요소로 구성  
- ACF/PACF는 AR·MA 차수 결정에 필수  
- 정상성 확보가 ARIMA 모델의 핵심  
- SARIMA는 계절적 패턴 고려  
- 예측 정확도는 RMSE·MAPE 등으로 측정  




# Chapter 10. 회귀분석(Regression Analysis)의 기초

회귀분석(Regression Analysis)은  
독립변수(X)가 종속변수(Y)에 어떤 영향을 미치는지 분석하는 통계 기법이다.

예:
- 광고비(X)가 매출(Y)에 어떤 영향을 주는가?  
- 공부시간(X)이 시험 점수(Y)에 미치는 영향은?  
- 나이·학력·경력이 연봉(Y)에 어떤 기여를 하는가?  
- 주택 면적·층수·위치가 가격(Y)에 어떤 영향을 주는가?

회귀분석은 ADP 필기·실기와 실무 데이터 분석에서 가장 핵심적인 도구이다.

---

# 🟥 10.1 단순선형회귀(Simple Linear Regression)

단 하나의 독립변수 X를 사용해 Y를 예측하는 방식.

모형 식:

\[
Y = \beta_0 + \beta_1 X + \epsilon
\]

- β₀ : 절편(intercept)  
- β₁ : 기울기(slope), X가 Y에 미치는 영향  
- ε : 오차항(error)

---

## 🔷 10.1.1 기울기 β₁의 해석

β₁ > 0 → X 증가 → Y 증가  
β₁ < 0 → X 증가 → Y 감소

예:
- 광고비 1만원 증가 시 매출 3만원 증가  
- 공부시간 1시간 증가 시 점수 2점 증가  

ADP에서 자주 묻는 질문:
> “기울기가 통계적으로 유의미한가?”  
→ t-검정으로 검정

---

## 🔷 10.1.2 최소제곱법(Ordinary Least Squares, OLS)

회귀계수는 OLS로 추정한다.

목표:
\[
\sum (y_i - \hat{y_i})^2 \quad \text{최소화}
\]

즉, 실제값과 예측값의 차이를 최소화하는 선을 찾는다.

---

# 🟥 10.2 회귀계수의 통계적 검정

각 회귀계수 β에 대해 다음을 검정한다.

\[
H_0 : \beta = 0 \\
H_1 : \beta \neq 0
\]

검정통계량:
\[
t = \frac{\hat{\beta}}{SE(\hat{\beta})}
\]

p-value < 0.05 → 유의한 변수  
p-value ≥ 0.05 → 영향 없음

이 검정은 회귀분석 해석에서 가장 중요한 부분이다.

---

# 🟥 10.3 결정계수 R²

회귀모형의 설명력을 나타내는 지표.

\[
R^2 = 1 - \frac{RSS}{TSS}
\]

0 ≤ R² ≤ 1  
값이 클수록 모델이 데이터를 잘 설명함.

주의:
- R²가 높아도 인과관계 보장 X  
- 독립변수를 많이 넣으면 자동으로 R² 증가 (overfitting 가능)

보완 지표 → **Adj. R² (수정 결정계수)**

---

# 🟥 10.4 다중회귀(Multiple Regression)

독립변수가 여러 개인 경우.

모형:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k + \epsilon
\]

예:
- 연봉 = 나이 + 경력 + 학력 + 성별  
- 주택가격 = 위치 + 면적 + 방수 + 준공연도  
- 고객평균매출 = 연령 + 구매빈도 + 방문경로 + 멤버십 등급  

다중회귀는 실무에서 가장 널리 사용되며  
ADP 시험에서도 매우 중요하다.

---

# 🟥 10.5 다중공선성(Multicollinearity)

독립변수들끼리 강한 상관관계가 있는 경우.

문제점:
- 회귀계수의 해석이 어려워짐  
- 표준오차 증가 → p-value 증가 → 유의하지 않게 보임  
- 계수가 방향성을 잃을 수 있음 (부호 뒤집힘)

진단방법:
- VIF (Variance Inflation Factor)  
  - VIF > 10 → 강한 다중공선성  
- 상관행렬 확인  
- 회귀계수 변동 확인

해결방법:
- 변수를 제거  
- PCA 같은 차원축소 사용  
- Ridge/Lasso 등 규제회귀 적용  

---

# 🟥 10.6 회귀모형의 가정 (ADP 필수 암기)

1) **선형성(Linearity)**  
   X와 Y 관계가 선형이어야 함.

2) **독립성(Independence)**  
   잔차가 서로 독립이어야 한다.  
   → Durbin-Watson으로 검정

3) **등분산성(Homoscedasticity)**  
   잔차의 분산이 일정해야 함.  
   → Residual plot 확인  
   → Breusch-Pagan test

4) **정규성(Normality)**  
   잔차가 정규분포를 따라야 회귀계수 검정이 정확해짐.  
   → Q-Q plot, Shapiro-Wilk

가정이 깨지면?
- 로그 변환  
- 제곱근 변환  
- Box-Cox 변환  
- 강건회귀(Robust Regression)

---

# 🟥 10.7 모형 선택(Model Selection)

모든 변수를 넣는 것이 좋은 모델은 아니다.

### 변수를 고르는 방법:

- 전진 선택(Forward Selection)  
- 후진 제거(Backward Elimination)  
- 단계적 선택(Stepwise Selection)

### 평가 지표:
- AIC (Akaike Information Criterion)  
- BIC (Bayesian Information Criterion)  
- Adj. R²  

AIC/BIC는 낮을수록 좋은 모델이다.

---

# 🟥 10.8 잔차 분석(Residual Analysis)

회귀모형이 적합했는지 확인하는 핵심 절차.

잔차 eᵢ = yᵢ − ŷᵢ

잔차플롯에서 확인해야 할 사항:

- 패턴이 있으면 안 된다 → 선형성 위배  
- 분산이 증가/감소하면 안 된다 → 등분산성 위배  
- 잔차가 정규성을 띄어야 한다 → QQ plot 확인  

잔차 분석이 잘 되었다면 모델 적합이 성공한 것이다.

---

# 🟥 10.9 과적합(Overfitting)과 편향-분산 트레이드오프

독립변수를 너무 많이 넣으면,
데이터를 너무 “정확하게” 설명하지만 실제 예측은 나빠짐.

과적합 해결:
- 변수 줄이기  
- 규제회귀(Ridge, Lasso)  
- 교차검증(Cross-validation)  
- 데이터 늘리기  

현대 머신러닝에서 가장 중요한 개념 중 하나다.

---

# 🟥 10.10 규제 회귀 (Ridge / Lasso Overview)

### ● Ridge Regression (L2 규제)
계수 크기에 패널티를 줌.

장점:
- 다중공선성 해결  
- 계수가 0이 되지는 않음  

### ● Lasso Regression (L1 규제)
불필요한 계수를 0으로 만드는 효과(변수 선택 기능).

실무에서는 elastic net(혼합형)도 많이 쓴다.

---

# ✔ Chapter 10 요약

- 회귀는 X가 Y에 미치는 영향을 분석하는 기법  
- 회귀계수는 t-검정으로 유의성 판단  
- R²는 설명력, Adj. R²는 과적합 방지  
- 다중공선성은 회귀모형의 큰 문제 → VIF로 확인  
- 회귀의 4대 가정: 선형성, 독립성, 등분산성, 정규성  
- 잔차 분석은 반드시 수행해야 함  
- 규제 회귀(Ridge/Lasso)는 현대 분석에서 매우 중요  

