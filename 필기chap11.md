# Chapter 11. 로지스틱 회귀(Logistic Regression)

로지스틱 회귀(Logistic Regression)는  
**종속변수가 이진 범주형(0/1, 구매/비구매, 이탈/잔존)**일 때 사용하는 대표적인 분류 모델이다.

회귀 모형이지만, 결과가 확률로 출력되기 때문에 분류 문제에서 널리 사용된다.

---

## 11.1 왜 로지스틱 회귀가 필요한가?

선형회귀를 사용하면 다음 문제가 발생한다:

- 예측값이 0~1 범위를 벗어남  
- 분류 경계가 직선으로 고정됨  
- 오차항 정규성 가정 위배  
- 분산이 일정하지 않음  

이 문제를 해결하기 위해 **시그모이드 함수(sigmoid)**를 활용하여  
예측값을 0~1 사이 확률로 변환한다.

---

## 11.2 로지스틱 함수(시그모이드)

로지스틱 회귀의 핵심은 다음 함수이다.

\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}
\]

- 입력값을 0~1 사이 확률로 변환  
- S자 형태의 완만한 곡선  
- 임계값(보통 0.5)을 기준으로 분류  

---

## 11.3 Logit(로그오즈) 변환

확률 p가 0~1 사이 값이라면,  
로그오즈(log-odds)는 실수 전체 범위를 가진다.

\[
logit(p) = \ln\left(\frac{p}{1 - p}\right)
\]

로지스틱 회귀는 **로그오즈가 X의 선형 함수**라는 가정 위에서 작동한다.

---

## 11.4 회귀계수의 해석

β₁ > 0 → X가 증가하면 사건 발생 확률 증가  
β₁ < 0 → X가 증가하면 확률 감소  

### 오즈비(Odds Ratio)

\[
OR = e^{\beta_1}
\]

- OR > 1 → 사건 발생 확률 증가  
- OR < 1 → 사건 발생 확률 감소  

예:  
β₁ = 0.405 → OR = e^0.405 ≈ 1.5  
→ X가 1 증가할 때 사건 발생 확률이 1.5배 증가

---

## 11.5 모델 적합도 평가

분류 모델 평가는 단순 정확도로 끝나지 않는다.

### ● 혼동행렬(Confusion Matrix)
- TP (True Positive)  
- TN (True Negative)  
- FP (False Positive)  
- FN (False Negative)

### ● Accuracy (정확도)

\[
Accuracy = \frac{TP + TN}{전체}
\]

### ● Precision (정밀도)
양성이라고 예측한 것 중 실제 양성 비율

### ● Recall (재현율)
실제 양성 중 모델이 맞힌 비율

### ● F1-score
정밀도와 재현율의 조화평균

### ● ROC-AUC  
AUC가 클수록 분류 성능 우수

이 지표들은 ADP 실기에도 필수적으로 등장한다.

---

## 11.6 다중공선성 문제

다중로지스틱 회귀에서는  
독립변수들 사이 상관관계가 높을 경우 문제가 된다.

문제점:
- 계수 해석 불가  
- 표준오차 증가 → p-value 증가  
- 모델 불안정

해결:
- 변수 제거  
- VIF(분산팽창계수) 확인  
- 규제 회귀(Ridge/Lasso) 사용  

---

## 11.7 장점과 한계

### 장점:
- 해석이 명확  
- 계산 효율적  
- 확률 출력 가능  
- 베이스라인 분류 모델로 우수  

### 한계:
- 비선형 경계에서 약함  
- 이상치(outlier)에 민감  
- 다중공선성의 영향 큼  
- 변수 스케일링 필요할 때가 있음  

---

## ✔ Chapter 11 요약

- 로지스틱 회귀는 확률 기반의 이진 분류 모델  
- 시그모이드 함수로 확률 출력  
- 로그오즈 기반 선형 모델  
- 오즈비(OR)는 해석의 핵심  
- 평가 지표: Accuracy, Precision, Recall, F1, ROC-AUC  
- 다중공선성·이상치에 주의  



# Chapter 12. 의사결정나무(Decision Tree) & 랜덤포레스트(Random Forest)

의사결정나무는 데이터를 조건에 따라 “분기(branch)”시키며  
최종 예측을 수행하는 비선형 모델이다.

머신러닝에서 가장 해석이 간단하고 직관적이기 때문에  
ADP 실기에서도 매우 자주 등장하는 모델이다.

---

# 🟥 12.1 의사결정나무의 특징

- 구조가 단순하고 시각적으로 해석 쉬움  
- 범주형·연속형 변수 모두 처리 가능  
- 스케일링(정규화/표준화) 필요 없음  
- 비선형 관계 학습 가능  
- 결측치 처리 용이

단점:
- 과적합(overfitting)에 매우 취약  
- 데이터가 조금만 변해도 트리가 크게 변동  
- 분기가 깊어질수록 일반화 성능 저하

---

# 🟥 12.2 트리 분할 기준 (Split Criteria)

## 🔷 12.2.1 분류(Classification) 트리
목표: 불순도(impurity)를 최소화하는 방향으로 분할

### ● 지니 불순도(Gini Impurity)
\[
G = 1 - \sum p_i^2
\]

### ● 엔트로피(Entropy)
\[
H = - \sum p_i \log_2 p_i
\]

지니 불순도와 엔트로피는 매우 유사하며  
실무에서는 지니 기준이 기본으로 많이 사용된다.

---

## 🔷 12.2.2 회귀(Regression) 트리
목표: 분산 감소

### ● 분산 기준
\[
Var = \frac{1}{n} \sum (y_i - \bar{y})^2
\]

각 노드에서 분산이 크게 줄어들수록 좋은 분할이다.

---

# 🟥 12.3 의사결정나무의 과적합 문제

트리는 데이터에 과도하게 맞춰지는 경향이 있다.  
(“데이터를 외워버리는” 모델)

과적합 해결 방법:

### ● 1) 가지치기(Pruning)
- 사전 가지치기(pre-pruning): 트리 깊이 제한  
- 사후 가지치기(post-pruning): 완성된 트리에서 가지 제거

### ● 2) 파라미터 제한
- max_depth  
- min_samples_split  
- min_samples_leaf  
- max_leaf_nodes  

### ● 3) 교차검증 활용

---

# 🟥 12.4 랜덤포레스트(Random Forest)

랜덤포레스트는 **여러 개의 결정트리를 무작위로 만들어 평균(또는 다수결)을 취하는 앙상블 모델**이다.

핵심 아이디어:
> 트리 하나는 약하지만,  
> 여러 트리를 묶으면 매우 강력한 모델이 된다.

---

## 12.4.1 어떻게 만드는가?

1) 부트스트랩 샘플링으로 데이터 부분집합 생성  
2) 각 트리에 대해 변수 일부만 사용하여 분할  
3) 다수결(분류) / 평균(회귀)으로 최종 예측

---

## 12.4.2 장점

- 과적합 감소  
- 예측 성능 우수  
- 변수 중요도 제공(feature importance)  
- 대용량 데이터에 강함  

---

## 12.4.3 단점

- 단일 트리에 비해 해석 어려움  
- 모델 크기 큼 → 속도 느릴 수 있음  
- 하이퍼파라미터 많음  

---

# 🟥 12.5 변수 중요도(Feature Importance)

랜덤포레스트는 변수의 중요도를 제공한다.

### ● Gini Importance
노드 분할 시 지니 감소량의 누적

### ● Permutation Importance
변수를 섞었을 때 모델 정확도 변화 측정

ADP에서도 “어떤 변수가 가장 중요한가?” 문제에서 자주 등장한다.

---

# 🟥 12.6 실무 활용 예

- 고객 이탈 예측  
- 대출 부도 예측  
- 질병 진단  
- 제품 불량 분류  
- 문서 분류  
- 구매 전환 예측  

의사결정나무는 빠른 해석이,  
랜덤포레스트는 강력한 성능이 장점이다.

---

# ✔ Chapter 12 요약

- 의사결정나무는 직관적이고 해석이 쉬운 모델  
- 지니/엔트로피로 불순도 감소를 기준으로 분할  
- 과적합에 취약 → 가지치기 필요  
- 랜덤포레스트는 여러 트리의 앙상블  
- 변수 중요도 산출 가능  
- 실무에서 매우 자주 사용되는 분류·회귀 모델



# Chapter 13. 클러스터 분석 (Cluster Analysis)

클러스터링(Clustering)은 **라벨이 없는 데이터(Unsupervised Learning)**를 
특성에 따라 자동으로 그룹화하는 분석 기법이다.

대표 활용 분야:
- 고객 세분화(Customer Segmentation)
- 이미지 분류
- 문서 군집화
- 이상치 탐지(Outlier Detection)
- 제품 추천 시스템

---

## 📌 13.1 클러스터링의 개념

클러스터링의 목적은 다음 두 가지이다:

1. **군집 내 데이터는 서로 유사하게**
2. **군집 간 데이터는 서로 다르게**

즉, "비슷한 것끼리 묶고, 다른 것은 멀리" 하는 구조를 자동으로 학습한다.

---

## 📌 13.2 거리(Distance) 개념

클러스터링은 거리 기반 알고리즘이므로, 거리 정의가 매우 중요하다.

### 🔸 대표적인 거리 측도
- **유클리드 거리(Euclidean)**
- **맨해튼 거리(Manhattan)**
- **코사인 거리(Cosine Distance)** → 텍스트 데이터에 자주 사용
- **마할라노비스 거리(Mahalanobis)** → 공분산을 고려하여 이상치 탐지에 강함

---

## 📌 13.3 K-means 클러스터링

가장 널리 쓰이는 대표적인 알고리즘.

### 🔧 알고리즘 절차
1. 클러스터 개수 **k**를 미리 정한다.
2. 임의로 k개의 초기 중심(centroid)을 선택한다.
3. 각 데이터를 가장 가까운 중심에 할당한다.
4. 중심을 다시 계산한다.
5. 중심 변화가 없을 때까지 반복한다.

### 📘 수식
각 점이 속한 중심과의 거리 제곱합(WCSS)을 최소화한다:

\[
\sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2
\]

---

## 📌 13.4 최적 클러스터 개수(k) 선택 방법

### 🔹 Elbow Method (엘보우 기법)
- 클러스터 수 k에 대해 SSE(오차 제곱합)를 계산
- **기울기가 갑자기 꺾이는 지점**이 최적의 k

### 🔹 Silhouette Score (실루엣 계수)
- **군집 내부 응집도**와 **군집 간 분리도** 평가
- 값이 1에 가까울수록 좋은 군집

---

## 📌 13.5 K-means의 장단점

### ✔ 장점
- 매우 빠름 (대규모 데이터에 적합)
- 구현 간단
- 결과 해석 쉬움

### ❌ 단점
- k를 미리 지정해야 함
- 이상치(outlier)에 매우 약함
- 구형(球形) 클러스터만 잘 분리됨
- 초기값에 따라 결과 달라짐

---

## 📌 13.6 계층적 군집화 (Hierarchical Clustering)

클러스터를 트리 구조(덴드로그램)로 만드는 방식.

### 🔧 종류
- **병합적(Agglomerative)**: 작은 군집 → 큰 군집으로 합침  
- **분할적(Divisive)**: 전체 집합 → 작은 군집으로 분할 (드묾)

### 🔧 병합적 군집화 방식
- **Single Linkage**: 가장 가까운 거리  
- **Complete Linkage**: 가장 먼 거리  
- **Average Linkage**: 평균 거리  
- **Ward’s Method**: 분산 기반 (가장 일반적)

### ✔ 장점
- k를 미리 정하지 않아도 됨  
- 덴드로그램으로 해석 쉬움  

### ❌ 단점
- 대규모 데이터에서 계산량 큼  
- 초기 병합 오류가 지속됨  

---

## 📌 13.7 DBSCAN (Density-Based Spatial Clustering)

밀도 기반 클러스터링 — 복잡한 모양의 군집 탐지 가능.

### 🔧 핵심 개념
- **Eps**: 반경  
- **MinPts**: 최소 이웃 수  

### 특징
- 노이즈(이상치)를 자동으로 분리  
- 클러스터 개수 k를 지정할 필요 없음  
- 비선형 군집에도 강력  

---

## 📌 13.8 클러스터링 평가 지표

지도학습처럼 "정답(label)"이 없기 때문에 다음 지표를 사용한다.

### ✔ 1) 실루엣 계수(Silhouette Score)
\[
-1 \le s \le 1
\]
높을수록 군집 분리가 잘 된 것.

### ✔ 2) CH 점수 (Calinski-Harabasz Index)
클러스터 간 분산 / 클러스터 내 분산  
높을수록 좋음.

### ✔ 3) Davies-Bouldin Index
DBI가 낮을수록 좋음.

---

## 📌 13.9 실무 활용 사례

### 🔸 고객 세분화 (CRM)
- 구매패턴 기반 VIP 고객 선정
- 타겟 마케팅 전략 수립

### 🔸 텍스트 문서 군집화
- 뉴스 기사 분류
- 토픽 모델링 기초 작업

### 🔸 이미지 패턴 분석
- 컴퓨터 비전 전처리 단계

### 🔸 이상치 탐지
- 금융 사기 탐지  
- 제조 공정 이상 탐지  

---

## 📌 13.10 주의해야 할 점

- 변수 스케일링(Standardization) 필수  
- 범주형 변수 → 원-핫 인코딩 필요  
- K-means는 outlier 제거 후 적용해야 안정적  
- 해석은 “의미 있는 군집명 부여”가 핵심  

---

# ✔ Chapter 13 요약

- K-means는 가장 널리 쓰이는 거리 기반 군집화  
- 최적 군집수(k)는 Elbow/Silhouette로 판단  
- 계층적 군집화는 트리 기반으로 시각화 강점  
- DBSCAN은 비선형·노이즈 포함 데이터에 강력  
- 군집 품질 평가는 Silhouette, DBI, CH 점수로 수행  
