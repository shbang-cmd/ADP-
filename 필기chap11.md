# Chapter 11. 로지스틱 회귀(Logistic Regression)

로지스틱 회귀(Logistic Regression)는  
**종속변수가 이진 범주형(0/1, 구매/비구매, 이탈/잔존)**일 때 사용하는 대표적인 분류 모델이다.

회귀 모형이지만, 결과가 확률로 출력되기 때문에 분류 문제에서 널리 사용된다.

---

## 11.1 왜 로지스틱 회귀가 필요한가?

선형회귀를 사용하면 다음 문제가 발생한다:

- 예측값이 0~1 범위를 벗어남  
- 분류 경계가 직선으로 고정됨  
- 오차항 정규성 가정 위배  
- 분산이 일정하지 않음  

이 문제를 해결하기 위해 **시그모이드 함수(sigmoid)**를 활용하여  
예측값을 0~1 사이 확률로 변환한다.

---

## 11.2 로지스틱 함수(시그모이드)

로지스틱 회귀의 핵심은 다음 함수이다.

\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}
\]

- 입력값을 0~1 사이 확률로 변환  
- S자 형태의 완만한 곡선  
- 임계값(보통 0.5)을 기준으로 분류  

---

## 11.3 Logit(로그오즈) 변환

확률 p가 0~1 사이 값이라면,  
로그오즈(log-odds)는 실수 전체 범위를 가진다.

\[
logit(p) = \ln\left(\frac{p}{1 - p}\right)
\]

로지스틱 회귀는 **로그오즈가 X의 선형 함수**라는 가정 위에서 작동한다.

---

## 11.4 회귀계수의 해석

β₁ > 0 → X가 증가하면 사건 발생 확률 증가  
β₁ < 0 → X가 증가하면 확률 감소  

### 오즈비(Odds Ratio)

\[
OR = e^{\beta_1}
\]

- OR > 1 → 사건 발생 확률 증가  
- OR < 1 → 사건 발생 확률 감소  

예:  
β₁ = 0.405 → OR = e^0.405 ≈ 1.5  
→ X가 1 증가할 때 사건 발생 확률이 1.5배 증가

---

## 11.5 모델 적합도 평가

분류 모델 평가는 단순 정확도로 끝나지 않는다.

### ● 혼동행렬(Confusion Matrix)
- TP (True Positive)  
- TN (True Negative)  
- FP (False Positive)  
- FN (False Negative)

### ● Accuracy (정확도)

\[
Accuracy = \frac{TP + TN}{전체}
\]

### ● Precision (정밀도)
양성이라고 예측한 것 중 실제 양성 비율

### ● Recall (재현율)
실제 양성 중 모델이 맞힌 비율

### ● F1-score
정밀도와 재현율의 조화평균

### ● ROC-AUC  
AUC가 클수록 분류 성능 우수

이 지표들은 ADP 실기에도 필수적으로 등장한다.

---

## 11.6 다중공선성 문제

다중로지스틱 회귀에서는  
독립변수들 사이 상관관계가 높을 경우 문제가 된다.

문제점:
- 계수 해석 불가  
- 표준오차 증가 → p-value 증가  
- 모델 불안정

해결:
- 변수 제거  
- VIF(분산팽창계수) 확인  
- 규제 회귀(Ridge/Lasso) 사용  

---

## 11.7 장점과 한계

### 장점:
- 해석이 명확  
- 계산 효율적  
- 확률 출력 가능  
- 베이스라인 분류 모델로 우수  

### 한계:
- 비선형 경계에서 약함  
- 이상치(outlier)에 민감  
- 다중공선성의 영향 큼  
- 변수 스케일링 필요할 때가 있음  

---

## ✔ Chapter 11 요약

- 로지스틱 회귀는 확률 기반의 이진 분류 모델  
- 시그모이드 함수로 확률 출력  
- 로그오즈 기반 선형 모델  
- 오즈비(OR)는 해석의 핵심  
- 평가 지표: Accuracy, Precision, Recall, F1, ROC-AUC  
- 다중공선성·이상치에 주의  



# Chapter 12. 의사결정나무(Decision Tree) & 랜덤포레스트(Random Forest)

의사결정나무는 데이터를 조건에 따라 “분기(branch)”시키며  
최종 예측을 수행하는 비선형 모델이다.

머신러닝에서 가장 해석이 간단하고 직관적이기 때문에  
ADP 실기에서도 매우 자주 등장하는 모델이다.

---

# 🟥 12.1 의사결정나무의 특징

- 구조가 단순하고 시각적으로 해석 쉬움  
- 범주형·연속형 변수 모두 처리 가능  
- 스케일링(정규화/표준화) 필요 없음  
- 비선형 관계 학습 가능  
- 결측치 처리 용이

단점:
- 과적합(overfitting)에 매우 취약  
- 데이터가 조금만 변해도 트리가 크게 변동  
- 분기가 깊어질수록 일반화 성능 저하

---

# 🟥 12.2 트리 분할 기준 (Split Criteria)

## 🔷 12.2.1 분류(Classification) 트리
목표: 불순도(impurity)를 최소화하는 방향으로 분할

### ● 지니 불순도(Gini Impurity)
\[
G = 1 - \sum p_i^2
\]

### ● 엔트로피(Entropy)
\[
H = - \sum p_i \log_2 p_i
\]

지니 불순도와 엔트로피는 매우 유사하며  
실무에서는 지니 기준이 기본으로 많이 사용된다.

---

## 🔷 12.2.2 회귀(Regression) 트리
목표: 분산 감소

### ● 분산 기준
\[
Var = \frac{1}{n} \sum (y_i - \bar{y})^2
\]

각 노드에서 분산이 크게 줄어들수록 좋은 분할이다.

---

# 🟥 12.3 의사결정나무의 과적합 문제

트리는 데이터에 과도하게 맞춰지는 경향이 있다.  
(“데이터를 외워버리는” 모델)

과적합 해결 방법:

### ● 1) 가지치기(Pruning)
- 사전 가지치기(pre-pruning): 트리 깊이 제한  
- 사후 가지치기(post-pruning): 완성된 트리에서 가지 제거

### ● 2) 파라미터 제한
- max_depth  
- min_samples_split  
- min_samples_leaf  
- max_leaf_nodes  

### ● 3) 교차검증 활용

---

# 🟥 12.4 랜덤포레스트(Random Forest)

랜덤포레스트는 **여러 개의 결정트리를 무작위로 만들어 평균(또는 다수결)을 취하는 앙상블 모델**이다.

핵심 아이디어:
> 트리 하나는 약하지만,  
> 여러 트리를 묶으면 매우 강력한 모델이 된다.

---

## 12.4.1 어떻게 만드는가?

1) 부트스트랩 샘플링으로 데이터 부분집합 생성  
2) 각 트리에 대해 변수 일부만 사용하여 분할  
3) 다수결(분류) / 평균(회귀)으로 최종 예측

---

## 12.4.2 장점

- 과적합 감소  
- 예측 성능 우수  
- 변수 중요도 제공(feature importance)  
- 대용량 데이터에 강함  

---

## 12.4.3 단점

- 단일 트리에 비해 해석 어려움  
- 모델 크기 큼 → 속도 느릴 수 있음  
- 하이퍼파라미터 많음  

---

# 🟥 12.5 변수 중요도(Feature Importance)

랜덤포레스트는 변수의 중요도를 제공한다.

### ● Gini Importance
노드 분할 시 지니 감소량의 누적

### ● Permutation Importance
변수를 섞었을 때 모델 정확도 변화 측정

ADP에서도 “어떤 변수가 가장 중요한가?” 문제에서 자주 등장한다.

---

# 🟥 12.6 실무 활용 예

- 고객 이탈 예측  
- 대출 부도 예측  
- 질병 진단  
- 제품 불량 분류  
- 문서 분류  
- 구매 전환 예측  

의사결정나무는 빠른 해석이,  
랜덤포레스트는 강력한 성능이 장점이다.

---

# ✔ Chapter 12 요약

- 의사결정나무는 직관적이고 해석이 쉬운 모델  
- 지니/엔트로피로 불순도 감소를 기준으로 분할  
- 과적합에 취약 → 가지치기 필요  
- 랜덤포레스트는 여러 트리의 앙상블  
- 변수 중요도 산출 가능  
- 실무에서 매우 자주 사용되는 분류·회귀 모델



# Chapter 13. 클러스터 분석 (Cluster Analysis)

클러스터링(Clustering)은 **라벨이 없는 데이터(Unsupervised Learning)**를 
특성에 따라 자동으로 그룹화하는 분석 기법이다.

대표 활용 분야:
- 고객 세분화(Customer Segmentation)
- 이미지 분류
- 문서 군집화
- 이상치 탐지(Outlier Detection)
- 제품 추천 시스템

---

## 📌 13.1 클러스터링의 개념

클러스터링의 목적은 다음 두 가지이다:

1. **군집 내 데이터는 서로 유사하게**
2. **군집 간 데이터는 서로 다르게**

즉, "비슷한 것끼리 묶고, 다른 것은 멀리" 하는 구조를 자동으로 학습한다.

---

## 📌 13.2 거리(Distance) 개념

클러스터링은 거리 기반 알고리즘이므로, 거리 정의가 매우 중요하다.

### 🔸 대표적인 거리 측도
- **유클리드 거리(Euclidean)**
- **맨해튼 거리(Manhattan)**
- **코사인 거리(Cosine Distance)** → 텍스트 데이터에 자주 사용
- **마할라노비스 거리(Mahalanobis)** → 공분산을 고려하여 이상치 탐지에 강함

---

## 📌 13.3 K-means 클러스터링

가장 널리 쓰이는 대표적인 알고리즘.

### 🔧 알고리즘 절차
1. 클러스터 개수 **k**를 미리 정한다.
2. 임의로 k개의 초기 중심(centroid)을 선택한다.
3. 각 데이터를 가장 가까운 중심에 할당한다.
4. 중심을 다시 계산한다.
5. 중심 변화가 없을 때까지 반복한다.

### 📘 수식
각 점이 속한 중심과의 거리 제곱합(WCSS)을 최소화한다:

\[
\sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2
\]

---

## 📌 13.4 최적 클러스터 개수(k) 선택 방법

### 🔹 Elbow Method (엘보우 기법)
- 클러스터 수 k에 대해 SSE(오차 제곱합)를 계산
- **기울기가 갑자기 꺾이는 지점**이 최적의 k

### 🔹 Silhouette Score (실루엣 계수)
- **군집 내부 응집도**와 **군집 간 분리도** 평가
- 값이 1에 가까울수록 좋은 군집

---

## 📌 13.5 K-means의 장단점

### ✔ 장점
- 매우 빠름 (대규모 데이터에 적합)
- 구현 간단
- 결과 해석 쉬움

### ❌ 단점
- k를 미리 지정해야 함
- 이상치(outlier)에 매우 약함
- 구형(球形) 클러스터만 잘 분리됨
- 초기값에 따라 결과 달라짐

---

## 📌 13.6 계층적 군집화 (Hierarchical Clustering)

클러스터를 트리 구조(덴드로그램)로 만드는 방식.

### 🔧 종류
- **병합적(Agglomerative)**: 작은 군집 → 큰 군집으로 합침  
- **분할적(Divisive)**: 전체 집합 → 작은 군집으로 분할 (드묾)

### 🔧 병합적 군집화 방식
- **Single Linkage**: 가장 가까운 거리  
- **Complete Linkage**: 가장 먼 거리  
- **Average Linkage**: 평균 거리  
- **Ward’s Method**: 분산 기반 (가장 일반적)

### ✔ 장점
- k를 미리 정하지 않아도 됨  
- 덴드로그램으로 해석 쉬움  

### ❌ 단점
- 대규모 데이터에서 계산량 큼  
- 초기 병합 오류가 지속됨  

---

## 📌 13.7 DBSCAN (Density-Based Spatial Clustering)

밀도 기반 클러스터링 — 복잡한 모양의 군집 탐지 가능.

### 🔧 핵심 개념
- **Eps**: 반경  
- **MinPts**: 최소 이웃 수  

### 특징
- 노이즈(이상치)를 자동으로 분리  
- 클러스터 개수 k를 지정할 필요 없음  
- 비선형 군집에도 강력  

---

## 📌 13.8 클러스터링 평가 지표

지도학습처럼 "정답(label)"이 없기 때문에 다음 지표를 사용한다.

### ✔ 1) 실루엣 계수(Silhouette Score)
\[
-1 \le s \le 1
\]
높을수록 군집 분리가 잘 된 것.

### ✔ 2) CH 점수 (Calinski-Harabasz Index)
클러스터 간 분산 / 클러스터 내 분산  
높을수록 좋음.

### ✔ 3) Davies-Bouldin Index
DBI가 낮을수록 좋음.

---

## 📌 13.9 실무 활용 사례

### 🔸 고객 세분화 (CRM)
- 구매패턴 기반 VIP 고객 선정
- 타겟 마케팅 전략 수립

### 🔸 텍스트 문서 군집화
- 뉴스 기사 분류
- 토픽 모델링 기초 작업

### 🔸 이미지 패턴 분석
- 컴퓨터 비전 전처리 단계

### 🔸 이상치 탐지
- 금융 사기 탐지  
- 제조 공정 이상 탐지  

---

## 📌 13.10 주의해야 할 점

- 변수 스케일링(Standardization) 필수  
- 범주형 변수 → 원-핫 인코딩 필요  
- K-means는 outlier 제거 후 적용해야 안정적  
- 해석은 “의미 있는 군집명 부여”가 핵심  

---

# ✔ Chapter 13 요약

- K-means는 가장 널리 쓰이는 거리 기반 군집화  
- 최적 군집수(k)는 Elbow/Silhouette로 판단  
- 계층적 군집화는 트리 기반으로 시각화 강점  
- DBSCAN은 비선형·노이즈 포함 데이터에 강력  
- 군집 품질 평가는 Silhouette, DBI, CH 점수로 수행  


# Chapter 14. 주성분분석(PCA) & 차원축소(Dimensionality Reduction)

PCA(Principal Component Analysis)는  
고차원 데이터를 **정보를 최대한 보존하면서 저차원으로 압축**하는 기법이다.

차원 축소는 다음과 같은 목적에서 활용된다:

- 시각화(2D, 3D로 변환)
- 노이즈 제거
- 다중공선성(multicollinearity) 해결
- 계산 성능 향상
- 특징(feature) 요약

---

## 📌 14.1 왜 차원축소가 필요한가?

### 🔸 1) 고차원 데이터 문제 (Curse of Dimensionality)
변수가 너무 많으면:
- 거리 계산이 의미를 잃음
- 모델 과적합 위험 증가
- 시각화 불가
- 계산량 폭증

### 🔸 2) 변수 간 중복 정보 존재
예: 키, 몸무게, BMI  
세 변수는 거의 동일한 정보를 가질 수 있음.

PCA는 이러한 중복 정보 제거에 매우 효과적이다.

---

## 📌 14.2 PCA 핵심 아이디어

PCA는 다음 질문을 해결한다:

> “데이터의 분산을 가장 잘 설명하는 새로운 축(Principal Component)을 만들 수 있을까?”

그 답은 **고유값 분해(Eigen Decomposition)** 와 **공분산 행렬**이다.

---

## 📌 14.3 PCA 알고리즘 단계

### 🔹 Step 1. 데이터 표준화 (Standardization)
\[
x' = \frac{x - \bar{x}}{s}
\]

PCA는 스케일에 민감하기 때문에 반드시 표준화해야 한다.

---

### 🔹 Step 2. 공분산 행렬 계산

\[
\Sigma = \frac{1}{n-1} X^T X
\]

공분산 행렬은 변수 간 연관성을 보여준다.

---

### 🔹 Step 3. 고유값(Eigenvalue) & 고유벡터(Eigenvector) 계산

- 고유값: 주성분이 설명하는 "분산 크기"
- 고유벡터: 주성분의 “방향”

고유값이 클수록 중요한 축이다.

---

### 🔹 Step 4. 주성분(Principal Components) 선택

설명 분산 비율(Cumulative Explained Variance Ratio)을 보고  
상위 k개 주성분을 선택한다.

예:
- PC1: 분산 55% 설명
- PC2: 추가로 30% 설명  
→ PC1 + PC2 = 전체 변동의 85% 설명 → 2차원으로 축소 가능

---

### 🔹 Step 5. 원래 데이터를 새로운 축에 투영

\[
Z = X W
\]

- X: 원래 데이터  
- W: 고유벡터 행렬  
- Z: 축소된 데이터  

---

## 📌 14.4 예시: PCA 직관적 이해

### 변수:
- 키  
- 몸무게  
- BMI  

이 세 변수는 고도로 상관되어 있다.  
PCA는 이를 다음처럼 축약할 수 있다:

- **PC1**: 체격을 나타내는 주요 요인 (분산 대부분 설명)
- **PC2**: 미세한 체형 변동(옆으로 퍼짐 정도)

3개 → 2개 또는 1개로 축소하더라도 정보 손실은 매우 적다.

---

## 📌 14.5 PCA의 해석

### 🔸 1) Scree Plot (스크리 도표)
각 주성분의 고유값을 시각화한 그래프  
→ 꺾이는 지점 “엘보우”가 적절한 주성분 수

### 🔸 2) Biplot
- PC1–PC2 좌표에 데이터 점을 표시  
- 변수 벡터도 함께 표시  
→ 어떤 변수가 주성분에 영향을 많이 주는지 시각적으로 확인 가능

---

## 📌 14.6 PCA의 장단점

### ✔ 장점
- 다중공선성 해결  
- 차원 축소로 과적합 감소  
- 계산 효율 향상  
- 시각화 쉬움  
- 노이즈 제거 효과

### ❌ 단점
- 해석이 어려울 수 있음 (PC1 = 무엇인가?)  
- 선형적인 차원축소 방법  
- 스케일링 필수  
- 이상치에 민감함

---

## 📌 14.7 PCA와 다른 차원축소 기법 비교

| 기법 | 특징 | 장점 | 단점 |
|------|------|------|------|
| **PCA** | 선형 | 빠름, 해석력 있음 | 비선형 구조 표현 못함 |
| **t-SNE** | 비선형 | 군집 시각화 강력 | 계산 느림, 매번 다른 결과 |
| **UMAP** | 비선형 | 속도 빠르고 성능 우수 | 파라미터 민감 |
| **LDA** | 지도학습 기반 | 클래스 분리를 최대로 | 클래스를 알고 있어야 함 |

---

## 📌 14.8 실무 적용 사례

### 🔸 고객 분석
- 수십 개의 행동 지표를 2~3개의 요인으로 축약  
- 마케팅 세그먼트 구성 용이

### 🔸 이미지 처리
- PCA 기반 Eigenface로 얼굴 인식

### 🔸 금융
- 수백 개의 자산을 몇 개의 요인(PC)으로 축약하여 리스크 분석

### 🔸 제조 공정
- 센서 다변량 데이터를 요약하여 이상 탐지

---

# ✔ Chapter 14 요약

- PCA는 공분산 기반 선형 차원축소 기법  
- 고유값·고유벡터로 주성분 생성  
- 데이터 스케일링 필수  
- Scree Plot / Explained Variance Ratio로 PC 수 결정  
- 실무에서 시각화, 노이즈 제거, 다중공선성 해결에 널리 사용  



# Chapter 15. 텍스트 마이닝(Text Mining)

텍스트 마이닝(Text Mining)은 **비정형 텍스트 데이터에서 의미를 추출하는 분석 기법**이다.  
자연어처리(NLP; Natural Language Processing)의 기본 단계로,  
ADP 실기에서도 자주 등장하는 주제이다.

활용 분야:
- 고객 리뷰 분석
- 감성 분석(Sentiment Analysis)
- 토픽 모델링
- 뉴스/문서 분류
- 채팅 데이터 분석
- SNS 여론 분석

---

## 📌 15.1 텍스트 데이터의 특징

텍스트는 숫자와 달리 다음과 같은 비정형적 속성을 가진다:

- 길이가 일정하지 않음  
- 단어 순서가 의미를 가짐  
- 오타·중의적 표현 존재  
- 의미 중복(시노님)  
- 노이즈가 매우 많음  

따라서 텍스트 분석에는 반드시 **전처리(NLP preprocessing)**가 필요하다.

---

## 📌 15.2 텍스트 전처리 단계

### 🔹 1) 토큰화(Tokenization)
텍스트를 단어 또는 형태소 단위로 분리  
예:  
"나는 밥을 먹었다" → ["나", "밥", "먹다"]

영어: 공백 기반 + nltk tokenizer  
한국어: 형태소 분석기(KoNLPy, Mecab 등) 필요

---

### 🔹 2) 정규화(Normalization)
- 소문자 변환
- 띄어쓰기 보정
- 오탈자 정정
- 특수문자/이모지 제거

---

### 🔹 3) 불용어(Stopwords) 제거
의미 없는 단어 제거.  
예:  
영어 → the, is, at, of, on  
한국어 → 은, 는, 이, 가, 하다, 되다

---

### 🔹 4) 어간추출(Stemming) / 표제어추출(Lemmatization)
- Stemming: 단순히 어근로 자르기  
- Lemmatization: 문법적 형태를 고려해 원형 반환  

예:  
studies, studying → study

---

### 🔹 5) 명사 추출(Noun Extraction)
한국어 분석에서 가장 중요한 단계 중 하나.  
(형태소 분석의 품질이 결과 품질을 결정한다)

---

## 📌 15.3 단어 빈도 분석 (TF, IDF)

### 🔹 1) Term Frequency (TF)
문서 내 단어 등장 빈도

\[
TF(t,d) = \frac{\text{단어 t 등장 수}}{\text{문서 전체 단어 수}}
\]

---

### 🔹 2) Inverse Document Frequency (IDF)

\[
IDF(t) = \log\frac{N}{df_t}
\]

- 자주 등장하는 단어의 영향력 감소  
- 희귀 단어의 중요성 강조  

---

### 🔹 3) TF-IDF

\[
TF\text{-}IDF = TF \times IDF
\]

문서 분류, 검색엔진, 추천시스템에 많이 사용된다.

---

## 📌 15.4 감성 분석 (Sentiment Analysis)

문장 또는 문서의 감정 상태를 분류하는 모델.

### 🔸 예시:
- 긍정 / 부정  
- 만족 / 불만  
- 긍정 / 중립 / 부정  

### 접근 방법:
1. **사전 기반 분석** (lexicon-based)
2. **머신러닝 모델**
3. **딥러닝 기반(BERT 등)**

---

## 📌 15.5 토픽 모델링 (Topic Modeling)

문서 집합에서 주요 주제(Topic)를 자동으로 추출하는 비지도학습 방법.

### 대표 알고리즘: **LDA (Latent Dirichlet Allocation)**

LDA 가정:

> 모든 문서는 여러 주제의 혼합이며  
> 각 주제는 단어 분포의 혼합이다.

LDA 출력:
- 문서별 주제 비율  
- 주제별 주요 단어  

---

## 📌 15.6 워드클라우드 시각화

단어 빈도수 기반으로 시각화된 구름 형태 이미지.  
가장 빈도 높은 단어가 크게 표시된다.

활용:
- 고객 리뷰 파악
- 설문 의견 분석
- SNS 트렌드 탐색

---

## 📌 15.7 임베딩(Embedding) 기법

텍스트를 벡터로 변환하여 모델 입력으로 사용.

### 🔸 대표 기법
- Bag-of-Words (BoW)
- TF-IDF 벡터
- Word2Vec (CBOW, Skip-gram)
- GloVe
- FastText
- BERT 임베딩

### 차이점:
| 방법 | 장점 | 단점 |
|------|------|------|
| BoW | 단순, 빠름 | 순서 정보 손실 |
| TF-IDF | 중요 단어 반영 | 문맥 정보 없음 |
| Word2Vec | 문맥 포함 | 긴 문서 표현 어려움 |
| BERT | 가장 강력, 문맥 완전 반영 | 계산 비용 큼 |

---

## 📌 15.8 텍스트 마이닝 실무 활용

### 🔸 고객 불만 분석
리뷰(부정 단어) 기반 제품·서비스 개선  

### 🔸 콜센터 VOC 분류
문의 유형 자동 분류 및 FAQ 자동 생성  

### 🔸 뉴스·SNS 모니터링
정치, 경제, 사회 트렌드 분석  

### 🔸 여론 분석
감성 점수 기반 긍·부정 여론 탐지  

### 🔸 금융 텍스트 분석
재무제표/뉴스 기반 주가 예측  

---

# ✔ Chapter 15 요약

- 텍스트는 비정형 데이터로서 NLP 전처리가 필수  
- 핵심 단계: 토큰화 → 정규화 → 불용어 제거 → 어간/표제어 추출  
- 분석 기법: TF-IDF, 감성 분석, 토픽 모델링(LDA)  
- 임베딩: Word2Vec, FastText, BERT  
- 실무 활용: VOC 분석, 여론 분석, 문서 분류, 추천 시스템  




